{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNRgjJZb6Ukj"
   },
   "source": [
    "# 1. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABW2yc1O6aKb"
   },
   "source": [
    "# 2. Dataset Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 2)\n",
      "Sentiment\n",
      "0    25000\n",
      "1    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/content/reviews.csv',encoding='latin-1',engine='python')\n",
    "df = df.dropna(subset=[\"Text\", \"Sentiment\"])\n",
    "df[\"Text\"] = df[\"Text\"].astype(str)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df[\"Sentiment\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i7O-Vya7H1T"
   },
   "source": [
    "# 3. Train–Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"Text\"].tolist(),\n",
    "    df[\"Sentiment\"].tolist(),\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df[\"Sentiment\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn7EHOpF7ViJ"
   },
   "source": [
    "# MODEL 1 — Full Fine-Tuning (DistilBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqfLuZ_H7iSE"
   },
   "source": [
    "4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d34500bbb743d1ae0b886e843d56ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486ede1853e74792824732eb8c5c0b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efdb3467d14438d82581ffa026b0765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b32ad6565d9457ea057537f15a1f76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_enc = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_enc   = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsoQ9HeG7rrT"
   },
   "source": [
    "# 5. Dataset Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict({\n",
    "    \"input_ids\": train_enc[\"input_ids\"],\n",
    "    \"attention_mask\": train_enc[\"attention_mask\"],\n",
    "    \"labels\": train_labels\n",
    "})\n",
    "\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"input_ids\": val_enc[\"input_ids\"],\n",
    "    \"attention_mask\": val_enc[\"attention_mask\"],\n",
    "    \"labels\": val_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skLk6hZe7wgQ"
   },
   "source": [
    "# 6. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac0f9d9193648e085c850f908d0ebf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "full_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8iaipNK8hLo"
   },
   "source": [
    "# 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_args = TrainingArguments(\n",
    "    output_dir=\"./full_ft_results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpgbzhem8Qua"
   },
   "source": [
    "# 8. Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBEhqyW88NxD"
   },
   "source": [
    "# 9. Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-491753130.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  full_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4376' max='4376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4376/4376 16:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.288635</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.877563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.359684</td>\n",
       "      <td>0.891467</td>\n",
       "      <td>0.890914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 00:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_trainer = Trainer(\n",
    "    model=full_model,\n",
    "    args=full_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "full_output = full_trainer.train()\n",
    "full_results = full_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1bJMuxF8t7T"
   },
   "source": [
    "# 10. Full Fine-Tuned Prediction (with Confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full(texts):\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = full_model(**enc)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "    return preds.cpu().numpy(), probs.max(dim=1).values.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWltWCIs83Au"
   },
   "source": [
    "# MODEL 2 — LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuOaUoy59AIA"
   },
   "source": [
    "# 11. Dataset Preparation for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df[\"Sentiment\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_ds_lora = Dataset.from_pandas(train_df)\n",
    "val_ds_lora   = Dataset.from_pandas(val_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvNxKfjz9HXs"
   },
   "source": [
    "# 12. Tokenization (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26802dff014e45a8b1aaa8b6eb128f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff3304bcbe4e0eab00508bac0b4874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"Text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_ds_lora = train_ds_lora.map(tokenize, batched=True)\n",
    "val_ds_lora   = val_ds_lora.map(tokenize, batched=True)\n",
    "\n",
    "train_ds_lora = train_ds_lora.rename_column(\"Sentiment\", \"labels\")\n",
    "val_ds_lora   = val_ds_lora.rename_column(\"Sentiment\", \"labels\")\n",
    "\n",
    "train_ds_lora.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_ds_lora.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38WSk_5m9SJg"
   },
   "source": [
    "# 13. LoRA Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n"
     ]
    }
   ],
   "source": [
    "lora_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(lora_base, lora_config)\n",
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb8V3wn99ey5"
   },
   "source": [
    "# 14. LoRA Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsFmjdW29p6m"
   },
   "source": [
    "# 15. LoRA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-200071499.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  lora_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4376' max='4376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4376/4376 05:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.309007</td>\n",
       "      <td>0.866467</td>\n",
       "      <td>0.869350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.304821</td>\n",
       "      <td>0.871067</td>\n",
       "      <td>0.873197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_args,\n",
    "    train_dataset=train_ds_lora,\n",
    "    eval_dataset=val_ds_lora,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "lora_output = lora_trainer.train()\n",
    "lora_results = lora_trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdgtH3o5-Usg"
   },
   "source": [
    "# 16. LoRA Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lora(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(lora_model.device) for k,v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = lora_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    pred = torch.argmax(probs).item()\n",
    "    confidence = probs[0][pred].item()\n",
    "    return pred, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQAobY76-7Bt"
   },
   "source": [
    "# Model Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Fine-Tuning Time\n",
    "full_train_time = full_output.metrics[\"train_runtime\"]\n",
    "full_acc = full_results[\"eval_accuracy\"]\n",
    "full_f1 = full_results[\"eval_f1\"]\n",
    "\n",
    "# LoRA training time\n",
    "lora_train_time = lora_output.metrics[\"train_runtime\"]\n",
    "lora_acc = lora_results[\"eval_accuracy\"]\n",
    "lora_f1 = lora_results[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_total_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "full_trainable = count_trainable_params(full_model)\n",
    "full_total = count_total_params(full_model)\n",
    "\n",
    "lora_trainable = count_trainable_params(lora_model)\n",
    "lora_total = count_total_params(lora_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== MODEL COMPARISON SUMMARY ==========\n",
      "\n",
      "FULL FINE-TUNING (DistilBERT)\n",
      "---------------------------------\n",
      "Train Time           : 16.17 minutes\n",
      "Validation Accuracy  : 0.8915\n",
      "Validation F1 Score  : 0.8909\n",
      "Trainable Params     : 66,955,010\n",
      "Total Params         : 66,955,010\n",
      "\n",
      "LoRA FINE-TUNING\n",
      "---------------------------------\n",
      "Train Time           : 5.15 minutes\n",
      "Validation Accuracy  : 0.8711\n",
      "Validation F1 Score  : 0.8732\n",
      "Trainable Params     : 739,586\n",
      "Total Params         : 67,694,596\n",
      "\n",
      "EFFICIENCY GAINS\n",
      "---------------------------------\n",
      "Speedup Factor       : 3.14x faster\n",
      "Parameter Reduction  : 98.90% fewer trainable params\n",
      "\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Side-by-Side Comparison\n",
    "print(\"\\n========== MODEL COMPARISON SUMMARY ==========\\n\")\n",
    "\n",
    "print(\"FULL FINE-TUNING (DistilBERT)\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Train Time           : {full_train_time/60:.2f} minutes\")\n",
    "print(f\"Validation Accuracy  : {full_acc:.4f}\")\n",
    "print(f\"Validation F1 Score  : {full_f1:.4f}\")\n",
    "print(f\"Trainable Params     : {full_trainable:,}\")\n",
    "print(f\"Total Params         : {full_total:,}\")\n",
    "print()\n",
    "\n",
    "print(\"LoRA FINE-TUNING\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Train Time           : {lora_train_time/60:.2f} minutes\")\n",
    "print(f\"Validation Accuracy  : {lora_acc:.4f}\")\n",
    "print(f\"Validation F1 Score  : {lora_f1:.4f}\")\n",
    "print(f\"Trainable Params     : {lora_trainable:,}\")\n",
    "print(f\"Total Params         : {lora_total:,}\")\n",
    "print()\n",
    "\n",
    "print(\"EFFICIENCY GAINS\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Speedup Factor       : {full_train_time / lora_train_time:.2f}x faster\")\n",
    "print(f\"Parameter Reduction  : {(1 - lora_trainable/full_trainable)*100:.2f}% fewer trainable params\")\n",
    "\n",
    "print(\"\\n=============================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQW15jx2_5MD"
   },
   "source": [
    "# Sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIDENCE COMPARISON (Same Review)\n",
      "----------------------------------\n",
      "Full Fine-Tuning : NEGATIVE | Confidence: 0.9947\n",
      "LoRA Fine-Tuning : NEGATIVE | Confidence: 0.9578\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"First thing first, I am not a Salman hater. I went into this movie expecting another good experience, because that's exactly what Bajrangi Bhaijaan did for me. I was disappointed.\n",
    "\n",
    "This movie means well and wants us love our neighbors, but when the main man of the movie (this movie is 95% Salman) is unconvincing and at times, annoying, you just cant enjoy the movie. I liked what Salman did in Bajrangi. I didn't like what Salman did in this. I cant make up my mind on if he was trying too hard to be a convincing disabled childlike-man or not trying at all, because most of the times the camera was on Salman, he's making extremely silly and at times annoying distorted faces. I know kids and people with similar disabilities do that too, but what Salman was doing looked like he was making fun of such people, far from convincing us that he was one of them. Then again, making Salman play a disabled childlike man was never a good idea.\n",
    "\n",
    "This movie can get emotional (trying too hard at times) so you might want to bring tissues along. The plot is pretty simple, Laxman aka Tubelight does little but wholesome things to increase his 'yakeen' which he believes will bring his brother back from war. The cinematography is breathtaking, but the war scenes were disappointing and poorly choreographed. The film's music was thoroughly enjoyable, especially Nach Meri Jaan and Radio.\n",
    "\n",
    "The supporting cast was pretty good. The late Om Puri showed us his excellence for one last time, acing the role as Banne Chacha. Sohail Khan played his part well, nothing else to it. Zhu Zhu was great whenever she was great on-screen, so was little Matin Rey Tangu. Zeeshan Ayyub was convincingly good in his role. SRK's cameo was (no pun intended) magical.\"\"\"\n",
    "\n",
    "\n",
    "# Full FT\n",
    "p_full, c_full = predict_full([test_text])\n",
    "\n",
    "# LoRA\n",
    "p_lora, c_lora = predict_lora(test_text)\n",
    "\n",
    "print(\"CONFIDENCE COMPARISON (Same Review)\")\n",
    "print(\"----------------------------------\")\n",
    "print(f\"Full Fine-Tuning : {'POSITIVE' if p_full[0]==1 else 'NEGATIVE'} | Confidence: {c_full[0]:.4f}\")\n",
    "print(f\"LoRA Fine-Tuning : {'POSITIVE' if p_lora==1 else 'NEGATIVE'} | Confidence: {c_lora:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIDENCE COMPARISON (Same Review)\n",
      "----------------------------------\n",
      "Full Fine-Tuning : POSITIVE | Confidence: 0.9716\n",
      "LoRA Fine-Tuning : POSITIVE | Confidence: 0.7267\n"
     ]
    }
   ],
   "source": [
    "neg_rev=\"I was absolutely mesmerized by the cinematography—the entire film felt like a dream, but one of those dreams where you're perpetually searching for something you can't quite name. The two-hour runtime was a masterclass in patience testing, proving that the writers are true masters of the drawn-out slow burn. Every single scene was remarkably consistent in its ability to underwhelm. The twist, when it finally arrived, was so brilliantly foreshadowed by a complete lack of context that I can only conclude the director's goal was ambitious, baffling boredom. I left the theatre feeling lighter, having successfully shed the weight of high expectations. Go see it.\"\n",
    "\n",
    "# Full FT\n",
    "p_full, c_full = predict_full([neg_rev])\n",
    "\n",
    "# LoRA\n",
    "p_lora, c_lora = predict_lora(neg_rev)\n",
    "\n",
    "print(\"CONFIDENCE COMPARISON (Same Review)\")\n",
    "print(\"----------------------------------\")\n",
    "print(f\"Full Fine-Tuning : {'POSITIVE' if p_full[0]==1 else 'NEGATIVE'} | Confidence: {c_full[0]:.4f}\")\n",
    "print(f\"LoRA Fine-Tuning : {'POSITIVE' if p_lora==1 else 'NEGATIVE'} | Confidence: {c_lora:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

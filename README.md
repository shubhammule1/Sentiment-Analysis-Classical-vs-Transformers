# ğŸ­ Sentiment Analysis: Classical ML vs Transformers
### Comparative study of classical ML models and transformer-based sentiment analysis, highlighting accuracy, efficiency, and LoRA fine-tuning gains.
## ğŸ“Œ Project Overview

This project presents a systematic **comparison** between **classical machine learning models** and **transformer-based** approaches for sentiment analysis on movie reviews.

Rather than focusing on accuracy alone, the study also evaluates training cost, inference time, parameter efficiency, and qualitative prediction behavior, revealing why higher accuracy does not always imply better sentiment understanding.
## ğŸ§ª Notebook Details
### ğŸ”¬ 01_Experimental_Benchmarking.ipynb

**Goal: Broad experimentation and failure-mode analysis**

**Models Explored:**
- Gaussian Naive Bayes
- Multinomial Naive Bayes
- Logistic Regression (CV)
- Linear SVM
- Random Forest
- XGBoost (baseline)

**Insight:**
Random Forest and XGBoost achieved ~85% accuracy but failed on long, sarcastic, and negation-heavy reviews, demonstrating that accuracy alone can be misleading for NLP tasks

### ğŸ“Š 02_Classical_Sentiment_Analysis.ipynb

**Goal: Clean, production-oriented classical NLP pipeline**

**Models Included:**
- Logistic Regression (CV)
- Linear SVM
- Multinomial Naive Bayes

GaussianNB is excluded due to incompatibility with sparse TF-IDF vectors.

### ğŸ¤— 03_BERT_and_LoRA.ipynb

**Goal: Accuracy vs efficiency in transformer fine-tuning**

**Techniques:**
- Full fine-tuning of **DistilBERT**
- Parameter-efficient fine-tuning using **LoRA (PEFT)**

## ğŸ§° Tech Stack
### Core:
- Python 3.9+
- NumPy, Pandas
### NLP & Preprocessing:
- NLTK (tokenization, stopwords, stemming)
- contractions
- TF-IDF Vectorization
### Classical Machine Learning:
- LogisticRegressionCV
- LinearSVC
- Multinomial Naive Bayes
- Gaussian Naive Bayes (experimental only)
- Random Forest (experimental only)
- XGBoost (experimental only)

### Deep Learning & Transformers:
- PyTorch
- Hugging Face Transformers
- DistilBERT (Sequence Classification)
### Parameter-Efficient Fine-Tuning:
- PEFT (LoRA)

## ğŸ“ˆ Model Performance Summary
### âœ… Classical Models

| Model                    | Accuracy (%) | 
|--------------------------|--------------
| Logistic Regression (CV) | **89.65**    |
| Linear SVM               | 89.18        | 
| Multinomial Naive Bayes  | 86.27        |

### ğŸ¤— Transformer Models
| Metric | Full Fine-Tuning (DistilBERT) | LoRA Fine-Tuning |
|:------ |:----------------------------:|:----------------:|
| Validation Accuracy | **89.11%** | 87.11% |
| F1 Score | **0.8906** | 0.8732 |
| Trainable Parameters | 66.9M | **739K** |
| Training Time | 14.92 minutes | **4.84 minutes** |

âœ… **98.9% parameter reduction**

âœ… **3.08Ã— faster training** with minimal performance loss
## ğŸ” Key Insights

- âœ… Logistic Regression remains a **strong and reliable NLP baseline**
- âš–ï¸ Tree-based models struggle with sparse high-dimensional text representations
- ğŸš€ LoRA delivers **near-BERT performance at a fraction of computational cost**
- ğŸ§  Evaluation must consider **error behavior**, not just aggregate metrics

## âœ… Conclusion
This repository demonstrates that **well-designed classical models remain competitive**, while **modern techniques like LoRA enable efficient scaling to transformer-level performance** under real-world constraints.






